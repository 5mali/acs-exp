{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPERIMENT 1: To demonstrate state space segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from figplot import figplot3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import socket\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_arg = int(sys.argv[1])\n",
    "# print (\"\\n\\nSeed ARG: \",seed_arg)\n",
    "seed_arg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedlist = np.array([161, 314, 228, 271828, 230, 4271031, 5526538, 6610165, 9849252, 34534, 73422, 8765])\n",
    "seed = seedlist[seed_arg]\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL :  B_ENO_TOKYO_13_40_53\n",
      "SEED  :  0\n",
      "HOST  :  19c994a59e65\n",
      "START :  2019-07-01 13:40:53.031422\n"
     ]
    }
   ],
   "source": [
    "LOCATION = 'tokyo'\n",
    "NAME       = 'B_ENO_'+ LOCATION.upper()+datetime.now().strftime(\"_%H_%M_%S\")\n",
    "MODELNAME  = NAME + '_' + str(seed) + '.pt'\n",
    "print(\"\\nMODEL : \", NAME)\n",
    "print(\"SEED  : \",seed_arg)\n",
    "print(\"HOST  : \",socket.gethostname())\n",
    "print(\"START : \",datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENO(object):\n",
    "    \n",
    "    #no. of forecast types is 6 ranging from 0 to 5\n",
    "  \n",
    "    def __init__(self, location='tokyo', year=2010, shuffle=False, day_balance=False):\n",
    "        self.location = location\n",
    "        self.year = year\n",
    "        self.day = None\n",
    "        self.hr = None\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.day_balance = day_balance\n",
    "\n",
    "        self.TIME_STEPS = None #no. of time steps in one episode\n",
    "        self.NO_OF_DAYS = None #no. of days in one year\n",
    "        \n",
    "        self.NO_OF_DAYTYPE = 10 #no. of daytypes\n",
    "        self.daycounter = 0 #to count number of days that have been passed\n",
    "        \n",
    "        self.sradiation = None #matrix with GSR for the entire year\n",
    "        self.senergy = None #matrix with harvested energy data for the entire year\n",
    "        self.fforecast = None #array with forecast values for each day\n",
    "        \n",
    "\n",
    "        self.henergy = None #harvested energy variable\n",
    "        self.fcast = None #forecast variable\n",
    "        self.sorted_days = [] #days sorted according to day type\n",
    "        \n",
    "        self.SMAX = 1000 # 1 Watt Solar Panel\n",
    "\n",
    "    \n",
    "    #function to get the solar data for the given location and year and prep it\n",
    "    def get_data(self):\n",
    "        #solar_data/CSV files contain the values of GSR (Global Solar Radiation in MegaJoules per meters squared per hour)\n",
    "        #weather_data/CSV files contain the weather summary from 06:00 to 18:00 and 18:00 to 06:00+1\n",
    "        location = self.location\n",
    "        year = self.year\n",
    "\n",
    "        THIS_DIR = getcwd()\n",
    "        SDATA_DIR = abspath(join(THIS_DIR, 'solar_data'))  #abspath(join(THIS_DIR, '../../..', 'data'))\n",
    "        \n",
    "        sfile = SDATA_DIR + '/' + location +'/' + str(year) + '.csv'\n",
    "        \n",
    "        #skiprows=4 to remove unnecessary title texts\n",
    "        #usecols=4 to read only the Global Solar Radiation (GSR) values\n",
    "        solar_radiation = pd.read_csv(sfile, skiprows=4, encoding='shift_jisx0213', usecols=[4])\n",
    "      \n",
    "        #convert dataframe to numpy array\n",
    "        solar_radiation = solar_radiation.values\n",
    "\n",
    "        #convert missing data in CSV files to zero\n",
    "        solar_radiation[np.isnan(solar_radiation)] = 0\n",
    "\n",
    "        #reshape solar_radiation into no_of_daysx24 array\n",
    "        solar_radiation = solar_radiation.reshape(-1,24)\n",
    "\n",
    "        if(self.shuffle): #if class instatiation calls for shuffling the day order. Required when learning\n",
    "            np.random.shuffle(solar_radiation) \n",
    "        self.sradiation = solar_radiation\n",
    "        \n",
    "        #GSR values (in MJ/sq.mts per hour) need to be expressed in mW\n",
    "        # Conversion is accomplished by \n",
    "        # solar_energy = GSR(in MJ/m2/hr) * 1e6 * size of solar cell * efficiency of solar cell /(60x60) *1000 (to express in mW)\n",
    "        # the factor of 2 in the end is assuming two solar cells\n",
    "        self.senergy = 2*self.sradiation * 1e6 * (55e-3 * 70e-3) * 0.15 * 1000/(60*60)\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    #function to map total day radiation into type of day ranging from 0 to 5\n",
    "    #the classification into day types is quite arbitrary. There is no solid logic behind this type of classification.\n",
    "    \n",
    "    def get_day_state(self,tot_day_radiation):\n",
    "        bin_edges = np.array([0, 3.5, 6.5, 9.0, 12.5, 15.5, 18.5, 22.0, 25, 28])\n",
    "        for k in np.arange(1,bin_edges.size):\n",
    "            if (bin_edges[k-1] <= tot_day_radiation <= bin_edges[k]):\n",
    "                day_state = k -1\n",
    "                break\n",
    "            else:\n",
    "                day_state = bin_edges.size - 1\n",
    "        return int(day_state)\n",
    "    \n",
    "    def get_forecast(self):\n",
    "        #create a perfect forecaster.\n",
    "        tot_day_radiation = np.sum(self.sradiation, axis=1) #contains total solar radiation for each day\n",
    "        get_day_state = np.vectorize(self.get_day_state)\n",
    "        self.fforecast = get_day_state(tot_day_radiation)\n",
    "        \n",
    "        #sort days depending on the type of day and shuffle them; maybe required when learning\n",
    "        for fcast in range(0,6):\n",
    "            fcast_days = ([i for i,x in enumerate(self.fforecast) if x == fcast])\n",
    "            np.random.shuffle(fcast_days)\n",
    "            self.sorted_days.append(fcast_days)\n",
    "        return 0\n",
    "    \n",
    "    def reset(self,day=0): #it is possible to reset to the beginning of a certain day\n",
    "        \n",
    "        self.get_data() #first get data for the given year\n",
    "        self.get_forecast() #calculate the forecast\n",
    "        \n",
    "        self.TIME_STEPS = self.senergy.shape[1]\n",
    "        self.NO_OF_DAYS = self.senergy.shape[0]\n",
    "        \n",
    "        self.day = day\n",
    "        self.hr = 0\n",
    "        \n",
    "        self.henergy = self.senergy[self.day][self.hr]\n",
    "        self.fcast = self.fforecast[self.day]\n",
    "        \n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        end_of_day = False\n",
    "        end_of_year = False\n",
    "        if not(self.day_balance): #if daytype balance is not required\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr] \n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.day < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.hr = 0\n",
    "                    self.day += 1\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else:\n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    \n",
    "        else: #when training, we want all daytypes to be equally represented for robust policy\n",
    "              #obviously, the days are going to be in random order\n",
    "            if(self.hr < self.TIME_STEPS - 1):\n",
    "                self.hr += 1\n",
    "                self.henergy = self.senergy[self.day][self.hr]\n",
    "                self.fcast = self.fforecast[self.day]\n",
    "            else:\n",
    "                if(self.daycounter < self.NO_OF_DAYS -1):\n",
    "                    end_of_day = True\n",
    "                    self.daycounter += 1\n",
    "                    self.hr = 0\n",
    "                    daytype = random.choice(np.arange(0,self.NO_OF_DAYTYPE)) #choose random daytype\n",
    "                    self.day = np.random.choice(self.sorted_days[daytype]) #choose random day from that daytype\n",
    "                    self.henergy = self.senergy[self.day][self.hr] \n",
    "                    self.fcast = self.fforecast[self.day]\n",
    "                else: \n",
    "                    end_of_day = True\n",
    "                    end_of_year = True\n",
    "                    self.daycounter = 0\n",
    "        \n",
    "        return [self.henergy, self.fcast, end_of_day, end_of_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAPM (object):\n",
    "    def __init__(self,location='tokyo', year=2010, shuffle=False, trainmode=False):\n",
    "\n",
    "        #all energy values i.e. BMIN, BMAX, BOPT, HMAX are in mWhr. Assuming one timestep is one hour\n",
    "        \n",
    "        self.BMIN = 50.0                #Minimum battery level that is tolerated. Maybe non-zero also\n",
    "        self.BMAX = 10000.0            #Max Battery Level. May not necessarily be equal to total batter capacity [3.6V x 2500mAh]\n",
    "        self.BOPT = 0.5 * self.BMAX    #Optimal Battery Level. Assuming 50% of battery is the optimum\n",
    "        self.BLIM_LO = 0.15*self.BMAX\n",
    "        self.BLIM_HI = 0.85*self.BMAX\n",
    "        self.BSAFE_LO = 0.35*self.BMAX\n",
    "        self.BSAFE_HI = 0.65*self.BMAX\n",
    "        \n",
    "        self.ENP_MARGIN = 0.25*self.BMAX\n",
    "\n",
    "        \n",
    "        self.HMIN = 0      #Minimum energy that can be harvested by the solar panel.\n",
    "        self.HMAX = None   #Maximum energy that can be harvested by the solar panel. [500mW]\n",
    "        \n",
    "        self.DMAX = 500      #Maximum energy that can be consumed by the node in one time step. [~ 3.6V x 135mA]\n",
    "        self.N_ACTIONS = 10  #No. of different duty cycles possible\n",
    "        self.DMIN = self.DMAX/self.N_ACTIONS #Minimum energy that can be consumed by the node in one time step. [~ 3.6V x 15mA]\n",
    "        \n",
    "        self.binit = None     #battery at the beginning of day\n",
    "        self.btrack = []      #track the battery level for each day\n",
    "        self.v_btrack = []      #track the virtual battery level for each day\n",
    "\n",
    "        self.atrack = []      #track the duty cycles for each day\n",
    "        self.htrack = []      #track the harvested for each day\n",
    "        self.batt = None      #battery variable\n",
    "        self.enp = None       #enp at end of hr\n",
    "        self.henergy = None   #harvested energy variable\n",
    "        self.fcast = None     #forecast variable\n",
    "        \n",
    "        self.MUBATT = 0.6\n",
    "        self.SDBATT = 0.02\n",
    "        \n",
    "        self.MUHENERGY = 0.5\n",
    "        self.SDHENERGY = 0.2\n",
    "        \n",
    "        self.MUENP = 0\n",
    "        self.SDENP = 0.02\n",
    "        \n",
    "        self.location  = location\n",
    "        self.year      = year\n",
    "        self.shuffle   = shuffle\n",
    "        self.trainmode = trainmode\n",
    "        self.eno       = None\n",
    "        \n",
    "        self.day_violation_flag = False\n",
    "        self.violation_flag     = False\n",
    "        self.violation_counter  = 0\n",
    "        self.batt_full_counter  = 0\n",
    "        self.batt_empty_counter = 0\n",
    "        \n",
    "        self.batt_violations    = 0\n",
    "\n",
    "        self.NO_OF_DAYTYPE      = 10 #no. of daytypes\n",
    " \n",
    "    def reset(self,day=0,batt=-1):\n",
    "        henergy, fcast, day_end, year_end = self.eno.reset(day) #reset the eno environment\n",
    "        self.violation_flag = False\n",
    "        self.violation_counter = 0\n",
    "        self.batt_violations = 0\n",
    "        \n",
    "        if(batt == -1):\n",
    "            self.batt = self.BOPT\n",
    "        else:\n",
    "            self.batt = batt\n",
    "            \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX)\n",
    "        self.binit = self.batt\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "        self.v_btrack = np.append(self.v_btrack, self.batt) #track battery levels\n",
    "#         self.enp = self.BOPT - self.batt\n",
    "        self.enp = self.binit - self.batt #enp is calculated\n",
    "        self.henergy = np.clip(henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "        self.fcast = fcast\n",
    "        \n",
    "        norm_batt    = self.batt    /self.BMAX\n",
    "        norm_enp     = self.enp     /self.BMAX\n",
    "        norm_henergy = self.henergy /self.HMAX\n",
    "        norm_fcast   = self.fcast   /(self.NO_OF_DAYTYPE-1)\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "\n",
    "        reward = 0\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]\n",
    "    \n",
    "    def getstate(self): #query the present state of the system\n",
    "        norm_batt    = self.batt    /self.BMAX\n",
    "        norm_enp     = self.enp     /self.BMAX\n",
    "        norm_henergy = self.henergy /self.HMAX\n",
    "        norm_fcast   = self.fcast   /(self.NO_OF_DAYTYPE-1)\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "\n",
    "        return c_state\n",
    "        \n",
    "    \n",
    "    #reward function\n",
    "    def rewardfn(self):\n",
    "        violation_penalty = 0\n",
    "        bmean = np.mean(self.btrack)\n",
    "        bdev = self.BOPT - bmean\n",
    "        if np.abs(bdev) > (self.BOPT - self.ENP_MARGIN):\n",
    "            reward = 1.5-np.abs(bdev)/self.BMAX*5\n",
    "        else:\n",
    "            reward = 1\n",
    "#         reward = 2 - 20*np.abs(bdev)/self.BMAX\n",
    "#         if(self.day_violation_flag):\n",
    "#             violation_penalty += 3    #penalty for violating battery limits anytime during the day\n",
    "        return np.clip(reward,-1,1)#(reward - violation_penalty)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        day_end = False\n",
    "        year_end = False\n",
    "        self.violation_flag = False\n",
    "        reward = 0\n",
    "        self.atrack = np.append(self.atrack, action+1) #track duty cycles\n",
    "        self.htrack = np.append(self.htrack, self.henergy)\n",
    "\n",
    "#         action_var = np.abs(np.mean(self.atrack) - action)/9 #can vary from 0 to 1\n",
    "#         reward += 0.25*(0.5 - action_var ) #reward penalizing high duty cycle variance [-0.5 to 0.5]*0.25\n",
    "      \n",
    "        action = np.clip(action, 0, self.N_ACTIONS-1) #action values range from (0 to N_ACTIONS-1)\n",
    "        e_consumed = (action+1)*self.DMAX/self.N_ACTIONS   #energy consumed by the node\n",
    "        \n",
    "        \n",
    "        self.batt += (self.henergy - e_consumed)\n",
    "        \n",
    "#         v_batt = self.binit + self.htrack.sum() - self.atrack.sum()*self.DMAX/self.N_ACTIONS\n",
    "#         self.v_btrack = np.append(self.v_btrack, v_batt) #track battery levels\n",
    "        \n",
    "        if(self.batt <= self.BMIN or self.batt >= self.BMAX ):\n",
    "                self.batt_violations += 1\n",
    "        \n",
    "        if(self.batt <= self.BMIN or self.batt >= self.BMAX ):\n",
    "#         if(self.batt < self.BLIM_LO or self.batt > self.BLIM_HI ):\n",
    "            self.violation_flag = True #penalty for violating battery limits everytime it happens\n",
    "#             reward -= 2\n",
    "#             if(self.batt < self.BLIM_LO): #battery depletion is more fatal than battery overflow\n",
    "#                 reward -= 2\n",
    "\n",
    "        if(self.batt <= self.BMIN ): \n",
    "            self.batt_empty_counter += 1\n",
    "        if(self.batt >= self.BMAX ): \n",
    "            self.batt_full_counter  += 1\n",
    "\n",
    "        if(self.violation_flag):\n",
    "            if(self.day_violation_flag == False): #penalty for violating battery limits anytime during the day - triggers once everyday\n",
    "                self.violation_counter += 1\n",
    "                self.day_violation_flag = True\n",
    "                \n",
    "        #calculate ENP before clipping\n",
    "#         self.enp = self.BOPT - self.batt\n",
    "        self.enp = self.binit - self.batt\n",
    "        \n",
    "        self.batt = np.clip(self.batt, self.BMIN, self.BMAX) #clip battery values within permitted level\n",
    "        self.btrack = np.append(self.btrack, self.batt) #track battery levels\n",
    "        \n",
    "        \n",
    "        #proceed to the next time step\n",
    "       \n",
    "        self.henergy, self.fcast, day_end, year_end = self.eno.step()\n",
    "        self.henergy = np.clip(self.henergy, self.HMIN, self.HMAX) #clip henergy within HMIN and HMAX\n",
    "                        \n",
    "        if(day_end): #if eno object flags that the day has ended then give reward\n",
    "            reward += self.rewardfn()\n",
    "#             if (self.trainmode): #reset battery to optimal level if limits are exceeded when training\n",
    "# #                 self.batt = np.random.uniform(self.DMAX*self.eno.TIME_STEPS/self.BMAX,0.8)*self.BMAX\n",
    "# #                 if (self.violation_flag):\n",
    "#                 if np.random.uniform() < HELP : #occasionaly reset the battery\n",
    "#                     self.batt = self.BOPT  \n",
    "            \n",
    "            self.day_violation_flag = False\n",
    "            self.binit = self.batt #this will be the new initial battery level for next day\n",
    "            self.btrack = [] #clear battery tracker\n",
    "#             self.v_btrack = [] #clear battery tracker\n",
    "            self.atrack = [] #clear duty cycle tracker\n",
    "            self.htrack = [] #clear henergy tracker\n",
    "   \n",
    "        norm_batt    = self.batt    /self.BMAX\n",
    "        norm_enp     = self.enp     /self.BMAX\n",
    "        norm_henergy = self.henergy /self.HMAX\n",
    "        norm_fcast   = self.fcast   /(self.NO_OF_DAYTYPE-1)\n",
    "        c_state = [norm_batt, norm_enp, norm_henergy, norm_fcast] #continuous states\n",
    "        \n",
    "        return [c_state, reward, day_end, year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPER PARAMS\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameters\n",
    "print(\"HYPER PARAMS\")\n",
    "BATCH_SIZE          = 32\n",
    "WT_DECAY            = None\n",
    "LR                  = 1e-4          # learning rate\n",
    "EPSILON             = 0.9           # greedy policy\n",
    "GAMMA               = 0.99           # reward discount\n",
    "LAMBDA              = 0.95          # parameter decay\n",
    "TARGET_REPLACE_ITER = 24*7*4*18     # target update frequency\n",
    "MEMORY_CAPACITY     = 24*7*4*12*3   # store upto six month worth of memory   \n",
    "\n",
    "N_ACTIONS           = 10            # no. of duty cycles (0,1,2,3,4)\n",
    "N_STATES            = 4             # number of state space parameter [batt, enp, henergy, fcast]\n",
    "\n",
    "HIDDEN_LAYER        = 50            # width of NN\n",
    "NO_OF_ITERATIONS    = 2\n",
    "GPU                 = False         # device\n",
    "HELP                = 0.05\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "#Class definitions for NN model and learning algorithm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.adv = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.adv.weight) \n",
    "    \n",
    "        self.val = nn.Linear(HIDDEN_LAYER, 1)\n",
    "        nn.init.xavier_uniform_(self.val.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        adv = self.adv(x)\n",
    "        val = self.val(x)\n",
    "        \n",
    "        return val + adv - adv.mean()\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        if(GPU): \n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.eval_net.to(device)\n",
    "        self.target_net.to(device)\n",
    "        self.device = device\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.per_memory_counter = 0                                         # for storing memory\n",
    "        self.per_memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2 + 1))     # initialize memory [mem: ([s], a, r, [s_]).priority ]\n",
    "#         self.memory_counter = 0                                         # for storing memory\n",
    "#         self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory [mem: ([s], a, r, [s_]) ]\n",
    "#         self.optimizer = torch.optim.RMSprop(self.eval_net.parameters(), lr=LR, alpha=0.95, eps=1e-2, weight_decay=0, momentum=0, centered=False)\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "\n",
    "        self.running_loss = 0.0\n",
    "        \n",
    "#         self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "#         self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR, weight_decay=WT_DECAY)\n",
    "#         self.loss_func = nn.SmoothL1Loss()\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.nettoggle = False\n",
    "        self.EXPLORE_FLAG = False\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] # return the argmax index\n",
    "        if np.random.uniform() > EPSILON:   # greedy\n",
    "#             action += np.random.randint(-3, 3)\n",
    "#             action = int(np.clip(action, 0, N_ACTIONS-1))\n",
    "            action = np.random.randint(0,N_ACTIONS)\n",
    "            self.EXPLORE_FLAG = True\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # input only one sample\n",
    "        if True:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            actions_value = actions_value.to(torch.device(\"cpu\"))\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.per_memory_counter % MEMORY_CAPACITY\n",
    "        self.per_memory[index, :] = transition\n",
    "        self.per_memory_counter += 1\n",
    "    \n",
    "    def get_priority(self,b_memory):\n",
    "        alpha = 0.6\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval   = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        a_eval   = self.eval_net(b_s).max(1)[1].view(24, 1) #best action according to eval_net\n",
    "        q_next   = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.gather(1, a_eval)   # shape (batch, 1)\n",
    "        error  = torch.abs(q_eval-q_target)\n",
    "        priority = torch.pow(error,0.6)\n",
    "        return priority\n",
    "        \n",
    "        \n",
    "    def store_day_transition(self, transition_rec):\n",
    "        data = transition_rec\n",
    "        priority = self.get_priority(transition_rec).data.numpy()\n",
    "#         print(priority)\n",
    "        per_data = np.column_stack((data,priority))\n",
    "\n",
    "        index = self.per_memory_counter % MEMORY_CAPACITY\n",
    "        self.per_memory= np.insert(self.per_memory, index, per_data,0)\n",
    "        self.per_memory_counter += transition_rec.shape[0]\n",
    "        \n",
    "#         index = self.memory_counter % MEMORY_CAPACITY\n",
    "#         self.memory= np.insert(self.memory, index, data,0)\n",
    "#         self.memory_counter += transition_rec.shape[0]\n",
    "    \n",
    "    def per_sample(self):\n",
    "        \n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            self.nettoggle = not self.nettoggle\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index_limit = min(MEMORY_CAPACITY, self.memory_counter)\n",
    "        \n",
    "        sample_index = np.random.choice(sample_index_limit, BATCH_SIZE)\n",
    "        \n",
    "        \n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        b_s = b_s.to(self.device)\n",
    "        b_a = b_a.to(self.device)\n",
    "        b_r = b_r.to(self.device)\n",
    "        b_s_ = b_s_.to(self.device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval   = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        a_eval   = self.eval_net(b_s).max(1)[1].view(BATCH_SIZE, 1) #best action according to eval_net\n",
    "        q_next   = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.gather(1, a_eval)   # shape (batch, 1)\n",
    "        loss     = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.running_loss += loss.mean().item()\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            print('LOSS : %.3f' %(self.running_loss / TARGET_REPLACE_ITER))\n",
    "            self.running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.5\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.5\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.55\n",
    "    \n",
    "    MU_FCAST = 0.5\n",
    "    SD_FCAST = 0.6\n",
    "    \n",
    "    norm_batt, norm_enp, norm_henergy, norm_fcast = s\n",
    "    \n",
    "    std_batt    = (norm_batt    - MU_BATT    )/SD_BATT\n",
    "    std_enp     = (norm_enp     - MU_ENP     )/SD_ENP\n",
    "    std_henergy = (norm_henergy - MU_HENERGY )/SD_HENERGY\n",
    "    std_fcast   = (norm_fcast   - MU_FCAST   )/SD_FCAST\n",
    "\n",
    "    return [std_batt, std_enp, std_henergy, std_fcast]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_destdize(s):\n",
    "    MU_BATT = 0.5\n",
    "    SD_BATT = 0.5\n",
    "    \n",
    "    MU_ENP = 0\n",
    "    SD_ENP = 0.5\n",
    "    \n",
    "    MU_HENERGY = 0.35\n",
    "    SD_HENERGY = 0.55\n",
    "    \n",
    "    MU_FCAST = 0.5\n",
    "    SD_FCAST = 0.6\n",
    "     \n",
    "    MU = np.array([MU_BATT, MU_ENP, MU_HENERGY, MU_FCAST])\n",
    "    SD = np.array([SD_BATT, SD_ENP, SD_HENERGY, SD_FCAST])\n",
    "        \n",
    "    X = s*SD[None, :]\n",
    "    return X + MU[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMIN       =  50.0\n",
      "BMAX       =  10000.0\n",
      "BOPT       =  5000.0\n",
      "ENP_MARGIN =  2500.0\n"
     ]
    }
   ],
   "source": [
    "#CONSTANTS\n",
    "LOCATION = 'tokyo'\n",
    "YEAR = 2010\n",
    "capm      = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "print(\"BMIN       = \", capm.BMIN)\n",
    "print(\"BMAX       = \", capm.BMAX)\n",
    "print(\"BOPT       = \", capm.BOPT)\n",
    "# print(\"BLIM_LO    = \", capm.BLIM_LO)\n",
    "# print(\"BLIM_HI    = \", capm.BLIM_HI)\n",
    "# print(\"BSAFE_LO   = \", capm.BSAFE_LO)\n",
    "# print(\"BSAFE_HI   = \", capm.BSAFE_HI)\n",
    "print(\"ENP_MARGIN = \", capm.ENP_MARGIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_1c(0)            =  -1.00\n",
      "RF_1c(ENP_MARGIN/4) =  -1.00\n",
      "RF_1c(ENP_MARGIN/2) =  -1.00\n",
      "RF_1c(ENP_MARGIN)   =  -1.00\n",
      "\n",
      "RF_1c(BMAX/2) =  -1.00\n",
      "RF_1c(BMAX)   =  -1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PLOT REWARD FUNCTIONS\n",
    "x = np.arange(0,capm.BMAX)\n",
    "y = np.array([capm.rewardfn() for capm.btrack in x])\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('REWARD FUNCTION 1c')\n",
    "ax.set_xlabel('BMEAN')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.axvline((capm.ENP_MARGIN)/capm.BMAX, color='g',alpha=0.75, linestyle='--')\n",
    "ax.axvline(1-(capm.ENP_MARGIN)/capm.BMAX, color='g',alpha=0.75, linestyle='--')\n",
    "\n",
    "# ax.axvline((capm.BSAFE_LO)/capm.BMAX, color='b',alpha=0.5, linestyle=':')\n",
    "# ax.axvline((capm.BSAFE_HI)/capm.BMAX, color='b',alpha=0.5, linestyle=\":\")\n",
    "ax.axvline((capm.BLIM_LO)/capm.BMAX, color='r',alpha=0.5, linestyle=':')\n",
    "ax.axvline((capm.BLIM_HI)/capm.BMAX, color='r',alpha=0.5, linestyle=\":\")\n",
    "ax.plot(x/capm.BMAX,y)\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "\n",
    "capm.v_btrack = 0;            print(\"RF_1c(0)            = {:6.2f}\".format(capm.rewardfn()) )\n",
    "capm.v_btrack = capm.ENP_MARGIN/4; print(\"RF_1c(ENP_MARGIN/4) = {:6.2f}\".format(capm.rewardfn()) )\n",
    "capm.v_btrack = capm.ENP_MARGIN/2; print(\"RF_1c(ENP_MARGIN/2) = {:6.2f}\".format(capm.rewardfn()) )\n",
    "capm.v_btrack = capm.ENP_MARGIN;   print(\"RF_1c(ENP_MARGIN)   = {:6.2f}\".format(capm.rewardfn()) )\n",
    "print(\"\");\n",
    "capm.v_btrack = capm.BMAX/2; print(\"RF_1c(BMAX/2) = {:6.2f}\".format(capm.rewardfn()) )\n",
    "capm.v_btrack = capm.BMAX;   print(\"RF_1c(BMAX)   = {:6.2f}\".format(capm.rewardfn()) )\n",
    "print(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING STARTS\n",
    "tic = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DQN' object has no attribute 'memory_counter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0d1e8c9c9a83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_memory_counter\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c29c41eed4b8>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# sample batch transitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0msample_index_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMEMORY_CAPACITY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0msample_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_index_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mb_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DQN' object has no attribute 'memory_counter'"
     ]
    }
   ],
   "source": [
    "#TRAIN \n",
    "dqn = DQN()\n",
    "# # for recording weights\n",
    "# oldfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "# old2fc1 = oldfc1\n",
    "\n",
    "# # oldfc2 = dqn.eval_net.fc2.weight.data.cpu().numpy().flatten()\n",
    "# # old2fc2 = oldfc2\n",
    "\n",
    "# # oldfc3 = dqn.eval_net.fc3.weight.data.cpu().numpy().flatten()\n",
    "# # old2fc3 = oldfc3\n",
    "\n",
    "# oldout = dqn.eval_net.fc_out.weight.data.cpu().numpy().flatten()\n",
    "# old2out = oldout\n",
    "# ########################################\n",
    "\n",
    "change_hr = 0 #when dqn target_net is updated by eval_net\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "batt_violation_rec = []\n",
    "day_violation_rec = []\n",
    "\n",
    "explore_action_space_coverage = []\n",
    "EXPLORE_FLAG = False\n",
    "\n",
    "print('Device: ', dqn.device)\n",
    "#TRAINING STARTS\n",
    "tic = datetime.now()\n",
    "\n",
    "e_rec = []\n",
    "lr_rec = []\n",
    "\n",
    "TOTAL_MEMORY = []\n",
    "\n",
    "LOCATION    = 'tokyo'#random.choice(['tokyo','aomori','minamidaito','fukuoka'])\n",
    "YEAR        = 2000 #random.choice(np.arange(2000,2010))\n",
    "\n",
    "capm        = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "capm.eno    = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "capm.HMAX   = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "s, r, day_end, year_end = capm.reset()\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "\n",
    "    # EPSILON SCHEDULING\n",
    "    e_slope = 0.1\n",
    "    e_start = 0.1\n",
    "    EPSILON = np.clip(e_start + e_slope*iteration,0.1,0.99)\n",
    "    e_rec  = np.append(e_rec,EPSILON)\n",
    "\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "    yr_record      = np.empty(4) #records hourly values of battery, henergy, reward and action \n",
    "\n",
    "    while True:\n",
    "        s0, s1, s2, s3 = stdize(s)\n",
    "        \n",
    "        a = dqn.choose_action(stdize(s))\n",
    "\n",
    "        if dqn.EXPLORE_FLAG:\n",
    "            explore_action_space_coverage.append(a)\n",
    "            dqn.EXPLORE_FLAG = False\n",
    "        \n",
    "        yr_record = np.vstack((yr_record, [s[0],s[2],r, a]))\n",
    "        TOTAL_MEMORY.append([s,a,r])\n",
    "        \n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "        \n",
    "        temp_transitions                = np.hstack((stdize(s), [a, r], stdize(s_)))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "#             transition_rec[:,5]  = r #broadcast reward to all states\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.per_memory_counter > 24*7:\n",
    "            dqn.learn()\n",
    "\n",
    "\n",
    "        if (year_end):\n",
    "            capm.eno.reset()\n",
    "            break\n",
    "\n",
    "        # transition to new state\n",
    "        s = s_\n",
    "        \n",
    "    yr_record = np.delete(yr_record, 0, 0)     #remove the first row which is garbage\n",
    "    hourly_yr_reward_rec = yr_record[:,2]      #extract reward information from the record array\n",
    "    yr_reward_rec = hourly_yr_reward_rec[::24] #only consider terminal rewards\n",
    "\n",
    "\n",
    "    print('\\nIteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    \n",
    "# # ########################################################################################### \n",
    "#   PLOT battery levels, hourly rewards and the weights\n",
    "    \n",
    "    #PLOT BATTERY AND REWARD\n",
    "    fig = plt.figure(figsize=(24,3))\n",
    "    TIME_STEPS = capm.eno.TIME_STEPS\n",
    "    NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "    DAY_SPACING = 15\n",
    "    TICK_SPACING = TIME_STEPS*DAY_SPACING\n",
    "    \n",
    "    #plot battery\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS),  yr_record[:,0],'r')\n",
    "    ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS), np.ones_like(yr_record[:,0])*capm.BOPT/capm.BMAX, 'k--', alpha=0.5)\n",
    "    ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS), np.ones_like(yr_record[:,0])*capm.BLIM_LO/capm.BMAX, 'r:')\n",
    "    ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS), np.ones_like(yr_record[:,0])*capm.BLIM_HI/capm.BMAX, 'r:')\n",
    "    ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS), np.ones_like(yr_record[:,0])*(capm.BOPT+capm.ENP_MARGIN /2) /capm.BMAX, 'g:')\n",
    "    ax.plot(np.arange(0,TIME_STEPS*NO_OF_DAYS), np.ones_like(yr_record[:,0])*(capm.BOPT-capm.ENP_MARGIN/2)/capm.BMAX, 'g:')\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.axvline(x=change_hr)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(TICK_SPACING))\n",
    "\n",
    "    #plot hourly reward\n",
    "    ax0 = ax.twinx()\n",
    "    ax0.plot(hourly_yr_reward_rec, color='y',alpha=0.4)\n",
    "    ax0.set_ylim(-4,2)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "# # ###########################################################################################\n",
    "# ###########################################################################################\n",
    "# PLOT STATE-SPACE COVERAGE\n",
    "#     from figplot import figplot\n",
    "#     figplot(state_space_coverage, action_space_coverage, explore_action_space_coverage)\n",
    "# # #################################################################\n",
    "#     # PLOT WEIGHTS\n",
    "#     fig = plt.figure(figsize=(18,6))\n",
    "\n",
    "#     ax1 = fig.add_subplot(211)\n",
    "#     newfc1 = dqn.eval_net.fc1.weight.data.cpu().numpy().flatten()\n",
    "#     xaxis = np.arange(0,newfc1.shape[0])\n",
    "#     ax1.bar(xaxis, old2fc1, color='b', alpha = 0.3)\n",
    "#     ax1.bar(xaxis, oldfc1,  color='b', alpha = 0.5)\n",
    "#     ax1.bar(xaxis, newfc1,  color='b', alpha = 1.0)\n",
    "#     ax1.set_title(\"FC1\")\n",
    "# #     ax1.set_ylim([-4,4])\n",
    "#     ax1.set_ylabel(\"Weight Value\")\n",
    "\n",
    "#     axO = fig.add_subplot(212)\n",
    "#     newout = dqn.eval_net.fc_out.weight.data.cpu().numpy().flatten()\n",
    "#     xaxis = np.arange(0,newout.shape[0])\n",
    "#     axO.bar(xaxis, old2out, color='g', alpha = 0.3)\n",
    "#     axO.bar(xaxis, oldout,  color='g', alpha = 0.5)\n",
    "#     axO.bar(xaxis, newout,  color='g', alpha = 1.0)\n",
    "#     axO.set_title(\"FC OUT\")\n",
    "# #     axO.set_ylim([-4,4])\n",
    "#     axO.set_ylabel(\"Weight Value\")\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()\n",
    "# # #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train time: {}\\n'.format(datetime.now() - tic))\n",
    "save_file = './models/'+ MODELNAME\n",
    "torch.save(dqn.eval_net.state_dict(), save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('beno',TOTAL_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################################################################################\n",
    "# ###########################################################################################\n",
    "\n",
    "# #PLOT LR AND EPSILON SCHEDULING\n",
    "# fig = plt.figure(figsize=(12,3))\n",
    "# ax = fig.add_subplot(111)\n",
    "# # ax.set_title(\"LR and EPSILON SCHEDULING V3\")\n",
    "# ax.set_xlabel(\"ITERATION\")\n",
    "# # ax.set_ylabel(\"LR\",      color = 'm')\n",
    "# ax.plot(lr_rec,          color = 'm', alpha = 0.3,label=\"LR\")\n",
    "# # ax.tick_params(axis='y', color = 'm')\n",
    "# # ax.ticklabel_format(style='sci', axis='y', scilimits=(-6,-7))\n",
    "# # ax.minorticks_on()\n",
    "\n",
    "# # ax.set_axisbelow(True)\n",
    "# ax.yaxis.set_ticklabels([])\n",
    "\n",
    "# ax.grid(b=True, which='minor', linestyle=':',  color='k', alpha = 0.1)\n",
    "# ax.grid(b=True, which='major', linestyle='-',  color='k', alpha = 0.1)\n",
    "\n",
    "# ax_ = ax.twinx()\n",
    "# # ax_.set_ylabel(\"EPSILON\", color = 'c')\n",
    "# # ax_.tick_params(axis='y', color = 'c')\n",
    "# # ax_.set_axisbelow(True)\n",
    "# # ax_.minorticks_on()\n",
    "# ax_.plot(e_rec,           color = 'c', alpha = 0.3, label=\"EPSILON\")\n",
    "# ax_.yaxis.set_ticklabels([])\n",
    "\n",
    "# # Plot the average reward log\n",
    "# ax1 = ax.twinx()\n",
    "# # ax1.set_ylabel(\"Terminal Reward\", color = 'b')\n",
    "# # ax1.set_ylim([-10,2]);\n",
    "# ax1.plot(avg_reward_rec,'b',label=\"ANNUAL REWARD\")\n",
    "# # ax1.tick_params(axis='y', colors='b')\n",
    "# ax1.yaxis.set_ticklabels([])\n",
    "\n",
    "# # Plot the violation record log\n",
    "# ax2 = ax.twinx()\n",
    "# # ax2.set_ylabel(\"Violations\",color = 'r')\n",
    "# ax2.plot(day_violation_rec,'r', label=\"DAY VIOLATIONS\")\n",
    "# ax2.plot(batt_violation_rec,'r',alpha=0.4, label=\"DOWNTIME\")\n",
    "# for xpt in np.argwhere(batt_violation_rec<1):\n",
    "#     ax2.axvline(x=xpt,color='g',linewidth='0.5')\n",
    "# # ax2.set_ylim([0,50]);\n",
    "# # ax2.tick_params(axis='y', colors='r')\n",
    "# ax2.yaxis.set_ticklabels([])\n",
    "\n",
    "# fig.legend(bbox_to_anchor=(1.1, 0.7))\n",
    "\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "# ###########################################################################################\n",
    "# ###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"ITERATION #0 :  LR = {:4.2e} @ EPSILON = {:4.2f}\".format(lr_rec[0],e_rec[0]))\n",
    "# print(\"ITERATION #10:  LR = {:4.2e} @ EPSILON = {:4.2f}\".format(lr_rec[10],e_rec[10]))\n",
    "# print(\"ITERATION #20:  LR = {:4.2e} @ EPSILON = {:4.2f}\".format(lr_rec[20],e_rec[20]))\n",
    "# print(\"ITERATION #50:  LR = {:4.2e} @ EPSILON = {:4.2f}\".format(lr_rec[50],e_rec[50]))\n",
    "# print(\"ITERATION #80:  LR = {:4.2e} @ EPSILON = {:4.2f}\".format(lr_rec[80],e_rec[80]))\n",
    "# print(\"ITERATION #99:  LR = {:4.2e} @ EPSILON = {:4.2f}\".format(lr_rec[99],e_rec[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_FILENAME = save_file\n",
    "print(\"Loading MODEL from FILE: \", S_FILENAME)\n",
    "dqn = DQN()\n",
    "dqn.eval_net.load_state_dict(torch.load(S_FILENAME))\n",
    "dqn.eval_net.eval()\n",
    "print(\"DEVICE: \", dqn.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION PHASE\n",
    "print(\"***MEASURING PERFORMANCE OF THE MODEL***\")\n",
    "for LOCATION in ['wakkanai','aomori','tokyo','fukuoka','minamidaito']:\n",
    "    results = np.empty(6)\n",
    "    for YEAR in np.arange(2000,2019):\n",
    "        capm      = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "        capm.eno  = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "        capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "        s, r, day_end, year_end = capm.reset()\n",
    "        yr_test_record = np.empty(4)\n",
    "\n",
    "        while True:\n",
    "            a = dqn.choose_greedy_action(stdize(s))\n",
    "            yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "            s_, r, day_end, year_end = capm.step(a)\n",
    "            if year_end:\n",
    "                break\n",
    "            s = s_\n",
    "\n",
    "        yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "        yr_test_reward_rec = yr_test_record[:,2]\n",
    "        yr_test_reward_rec = yr_test_reward_rec[::24] #annual average reward\n",
    "        results = np.vstack((results, [int(YEAR), np.mean(yr_test_reward_rec), int(capm.violation_counter), int(capm.batt_violations), int(capm.batt_empty_counter), int(capm.batt_full_counter)]))\n",
    "        print(\"\\n\\n************************************\")\n",
    "        print(\"Day Violations           = {:6d}\".format(capm.violation_counter))\n",
    "        print(\"Battery Limit Violations = {:6d}\".format(capm.batt_violations))\n",
    "        print(\"Battery FULL Violations  = {:6d}\".format(capm.batt_full_counter))\n",
    "        print(\"Battery EMPTY Violations = {:6d}\".format(capm.batt_empty_counter))\n",
    "    ###########################################################################################\n",
    "    ###########################################################################################\n",
    "        #     Plot the reward and battery for the entire year run\n",
    "        title = LOCATION.upper() + ',' + str(YEAR)\n",
    "        NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "\n",
    "        fig = plt.figure(figsize=(24,6))\n",
    "        fig.suptitle(title, fontsize=15)\n",
    "\n",
    "    #         ax1 = fig.add_subplot(211)\n",
    "    #         ax1.plot(yr_test_reward_rec)\n",
    "    #         ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "    #         ax1.set_ylim([-4,2])\n",
    "\n",
    "        ax2 = fig.add_subplot(211)\n",
    "        ax2.plot(yr_test_record[:,0],'r')\n",
    "        ax2.plot(np.ones_like(yr_test_record[:,0])*capm.BOPT/capm.BMAX, 'k--')\n",
    "        ax2.plot(np.ones_like(yr_test_record[:,0])*capm.BLIM_LO/capm.BMAX, 'r:')\n",
    "        ax2.plot(np.ones_like(yr_test_record[:,0])*capm.BLIM_HI/capm.BMAX, 'r:')\n",
    "        ax2.plot(np.ones_like(yr_test_record[:,0])*(capm.BOPT+capm.ENP_MARGIN /2) /capm.BMAX, 'g:')\n",
    "        ax2.plot(np.ones_like(yr_test_record[:,0])*(capm.BOPT-capm.ENP_MARGIN/2)/capm.BMAX, 'g:')\n",
    "\n",
    "        ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "        ax2.set_ylim([0,1])\n",
    "        plt.sca(ax2)\n",
    "        plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    ###########################################################################################\n",
    "    ###########################################################################################\n",
    "\n",
    "\n",
    "    results = np.delete(results,0,0)\n",
    "    print(\"\\n\")\n",
    "    print(LOCATION.upper())\n",
    "    print('YEAR\\tAVG_RWD\\t\\tVIOLATIONS\\tEMPTY\\tFULL')\n",
    "    print('\\t\\t\\tDAY\\tBATT')\n",
    "\n",
    "    for x in np.arange(0,results.shape[0]):\n",
    "        print('{}\\t {}\\t\\t{}\\t {}\\t{}\\t{}'.format(int(results[x,0]), np.around(results[x,1],2), int(results[x,2]), int(results[x,3]), int(results[x,4]),  int(results[x,5]) ))\n",
    "\n",
    "    print(\"\\nTOTAL Day  Violations:  \",np.sum(results[:, 2]))\n",
    "    print(\"TOTAL Batt  Violations: \",np.sum(results[:,3]))\n",
    "    print(\"TOTAL EMPTY Violations: \",np.sum(results[:,4]))\n",
    "    print(\"TOTAL FULL  Violations: \",np.sum(results[:,5]))\n",
    "    print(\"************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nRun time: {}'.format(datetime.now() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YEAR = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capm = CAPM(LOCATION,YEAR,shuffle=False, trainmode=False) #instantiate the CAPM class\n",
    "# capm.eno = ENO(LOCATION,YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "# capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "# s, r, day_end, year_end = capm.reset()\n",
    "# yr_test_record = np.empty(4)\n",
    "\n",
    "# while True:\n",
    "#     a = dqn.choose_greedy_action(stdize(s))\n",
    "\n",
    "#     #state = [batt, enp, henergy, fcast]\n",
    "#     yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "#     # take action\n",
    "#     s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "#     if year_end:\n",
    "#         break\n",
    "\n",
    "#     s = s_\n",
    "\n",
    "# yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "# yr_test_reward_rec = yr_test_record[:,2]\n",
    "# yr_test_reward_rec = yr_test_reward_rec[::24]\n",
    "\n",
    "# title = LOCATION.upper() + ',' + str(YEAR)\n",
    "# NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "\n",
    "# fig = plt.figure(figsize=(24,6))\n",
    "# fig.suptitle(title, fontsize=15)\n",
    "\n",
    "# #     ax1 = fig.add_subplot(211)\n",
    "# #     ax1.plot(yr_test_reward_rec)\n",
    "# #     ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "# #     ax1.set_ylim([-3,3])\n",
    "\n",
    "# #Plot the reward and battery for the entire year run\n",
    "# ax2 = fig.add_subplot(111)\n",
    "# ax2.plot(yr_test_record[:,0],'r')\n",
    "# ax2.plot(np.ones_like(yr_test_record[:,0])*capm.BOPT/capm.BMAX, 'k--')\n",
    "# ax2.plot(np.ones_like(yr_test_record[:,0])*capm.BLIM_LO/capm.BMAX, 'r:')\n",
    "# ax2.plot(np.ones_like(yr_test_record[:,0])*capm.BLIM_HI/capm.BMAX, 'r:')\n",
    "# ax2.plot(np.ones_like(yr_test_record[:,0])*(capm.BOPT+capm.ENP_MARGIN /2) /capm.BMAX, 'g:')\n",
    "# ax2.plot(np.ones_like(yr_test_record[:,0])*(capm.BOPT-capm.ENP_MARGIN/2)/capm.BMAX, 'g:')\n",
    "# ax2.set_title(\"\\n\\nYear Run Battery\")\n",
    "# ax2.set_ylim([0,1])\n",
    "# plt.sca(ax2)\n",
    "# plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAY_START = 320\n",
    "# DAY_END   = 325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot the reward and battery for the entire year run on a day by day basis\n",
    "# title = LOCATION.upper() + ',' + str(YEAR)\n",
    "# TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "# for DAY in range(DAY_START,DAY_END):\n",
    "#     START = DAY*24\n",
    "#     END = START+24\n",
    "\n",
    "#     daytitle = title + ' - DAY ' + str(DAY)\n",
    "#     fig = plt.figure(figsize=(16,4))\n",
    "#     st = fig.suptitle(daytitle)\n",
    "\n",
    "#     ax2 = fig.add_subplot(121)\n",
    "#     ax2.plot(yr_test_record[START:END,1],'g')\n",
    "#     ax2.set_title(\"HARVESTED ENERGY\")\n",
    "#     ax2.set_xlabel(\"Hour\")\n",
    "#     ax2.set_ylim([0,1.05])\n",
    "    \n",
    "#     ax2.text(0.1, 0.6, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "#     ax2.text(0.1, 0.5, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "#     ax2.text(0.1, 0.4, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "#     if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "#         ax2.text(0.1, 0.1, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "        \n",
    "#     #plot battery for year run\n",
    "#     ax1 = fig.add_subplot(122)\n",
    "#     ax1.plot(TIME_AXIS,              yr_test_record[START:END,0],'r')\n",
    "#     ax1.plot(TIME_AXIS, np.ones_like(yr_test_record[START:END,0])*capm.BOPT/capm.BMAX, 'r--')\n",
    "#     ax1.plot(TIME_AXIS, np.ones_like(yr_test_record[START:END,0])*capm.BLIM_LO/capm.BMAX, 'r-.',alpha=0.5)\n",
    "#     ax1.plot(TIME_AXIS, np.ones_like(yr_test_record[START:END,0])*capm.BLIM_HI/capm.BMAX, 'r-.',alpha=0.5)\n",
    "#     ax1.plot(TIME_AXIS, np.ones_like(yr_test_record[START:END,0])*(capm.BOPT+capm.ENP_MARGIN /2) /capm.BMAX, 'g:',alpha=0.5)\n",
    "#     ax1.plot(TIME_AXIS, np.ones_like(yr_test_record[START:END,0])*(capm.BOPT-capm.ENP_MARGIN/2)/capm.BMAX, 'g:',alpha=0.5)\n",
    "    \n",
    "#     ax1.set_title(\"YEAR RUN TEST\")\n",
    "#     ax1.set_xlabel(\"Hour\")\n",
    "#     ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "#     ax1.set_ylim([-0.05,1.05])\n",
    "\n",
    "#     #plot actions for year run\n",
    "#     ax1a = ax1.twinx()\n",
    "#     ax1a.plot(yr_test_record[START:END,3]+1)\n",
    "#     ax1a.set_ylim([0,N_ACTIONS])\n",
    "#     ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "#     ax1a.set_ylim([0,10.25])\n",
    "\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     st.set_y(0.95)\n",
    "#     fig.subplots_adjust(top=0.75)\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
